{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imutils import contours\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "TRAINING_PATH = 'training_data/'\n",
    "BATCH_SIZE = 16\n",
    "IMG_SHAPE = (64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "class RandomShift(object):\n",
    "    def __init__(self, shift):\n",
    "        self.shift = shift\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_params(shift):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        hshift, vshift = np.random.uniform(-shift, shift, size=2)\n",
    "\n",
    "        return hshift, vshift \n",
    "    def __call__(self, img):\n",
    "        hshift, vshift = self.get_params(self.shift)\n",
    "        \n",
    "        return img.transform(img.size, Image.AFFINE, (1,0,hshift,0,1,vshift), resample=Image.BICUBIC, fill=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorie mapping: {7: 'text', 0: 'down', 4: 'right', 6: 'right_down', 3: 'left_down', 8: 'up_right', 2: 'empty', 5: 'right_and_down', 1: 'down_right'}\n"
     ]
    }
   ],
   "source": [
    "# define basic image transforms for preprocessing\n",
    "transform = transforms.Compose(\n",
    "[\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(0.2),\n",
    "    RandomShift(0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = (0.5,), std = (0.5, ))\n",
    "])\n",
    "\n",
    "class CrosswordDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Custom Dataset object for the CDiscount competition\n",
    "        Parameters:\n",
    "            root_dir - directory including category folders with images\n",
    "\n",
    "        Example:\n",
    "        images/\n",
    "            1000001859/\n",
    "                26_0.jpg\n",
    "                26_1.jpg\n",
    "                ...\n",
    "            1000004141/\n",
    "                ...\n",
    "            ...\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.categories = sorted(os.listdir(root_dir))\n",
    "        self.cat2idx = dict(zip(self.categories, range(len(self.categories))))\n",
    "        self.idx2cat = dict(zip(self.cat2idx.values(), self.cat2idx.keys()))\n",
    "        self.files = []\n",
    "        cat_mapping = {}\n",
    "        for (dirpath, dirnames, filenames) in os.walk(self.root_dir):\n",
    "            for f in filenames:\n",
    "                if f.endswith('.png'):\n",
    "                    o = {}\n",
    "                    o['img_path'] = dirpath + '/' + f\n",
    "                    o['category'] = self.cat2idx[dirpath[dirpath.find('/')+1:]]\n",
    "                    cat_mapping[o['category']] = dirpath.split('/')[-1]\n",
    "                    self.files.append(o)\n",
    "        self.transform = transform\n",
    "        print(f'Categorie mapping: {cat_mapping}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]['img_path']\n",
    "        category = self.files[idx]['category']\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, IMG_SHAPE)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return {'image': image, 'category': category}\n",
    "\n",
    "\n",
    "# create instance of data class and pytorch dataloader\n",
    "dataSet = CrosswordDataset(TRAINING_PATH, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataSet, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Network created #########\n",
      "Architecture:\n",
      " Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=7744, out_features=512, bias=True)\n",
      "  (bnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (bnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (bnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n",
      "Started Training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_305739/3120566653.py:20: DeprecationWarning: AFFINE is deprecated and will be removed in Pillow 10 (2023-07-01). Use Transform.AFFINE instead.\n",
      "  return img.transform(img.size, Image.AFFINE, (1,0,hshift,0,1,vshift), resample=Image.BICUBIC, fill=1)\n",
      "/tmp/ipykernel_305739/3120566653.py:20: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  return img.transform(img.size, Image.AFFINE, (1,0,hshift,0,1,vshift), resample=Image.BICUBIC, fill=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    60] loss: 0.068\n",
      "[2,    60] loss: 0.035\n",
      "[3,    60] loss: 0.027\n",
      "[4,    60] loss: 0.022\n",
      "[5,    60] loss: 0.021\n",
      "[6,    60] loss: 0.016\n",
      "[7,    60] loss: 0.016\n",
      "[8,    60] loss: 0.015\n",
      "[9,    60] loss: 0.014\n",
      "[10,    60] loss: 0.013\n",
      "[11,    60] loss: 0.014\n",
      "[12,    60] loss: 0.011\n",
      "[13,    60] loss: 0.012\n",
      "[14,    60] loss: 0.016\n",
      "[15,    60] loss: 0.011\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    # Pytorch CNN model class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 5)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*11*11, 512)\n",
    "        self.bnorm1 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bnorm2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bnorm3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 9)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        \n",
    "        x = x.view(-1, 64*11*11)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bnorm1(self.fc1(x)))\n",
    "        x = F.relu(self.bnorm2(self.fc2(x)))\n",
    "        x = F.relu(self.bnorm3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print('######### Network created #########')\n",
    "print('Architecture:\\n', net)\n",
    "\n",
    "### Train\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "print('Started Training!')\n",
    "net.train()\n",
    "for epoch in range(15):\n",
    "    running_loss = 0.0\n",
    "    examples = 0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # Get the inputs\n",
    "        inputs, labels = data['image'], data['category']\n",
    "        \n",
    "        # Wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.data\n",
    "        examples += BATCH_SIZE\n",
    "    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / examples))\n",
    "\n",
    "print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save torch model for further prediction\n",
    "torch.save(net.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
